#  聚类分析

##  聚类分析的一般步骤

(1) 选择合适的变量 

(2) 缩放数据 

最常用的方法是将每个变量标准化为均值为0和标准差为1的变量。其他的替代方法包括每个变量被其最大值相除或该变量减去它的平均值并除以变量的平均绝对偏差。

~~~R
df1 <- apply(mydata, 2, function(x){(x-mean(x))/sd(x)})
df2 <- apply(mydata, 2, function(x){x/max(x)})
df3 <- apply(mydata, 2, function(x){(x – mean(x))/mad(x)})
~~~

`scale`与df1效果相当

(3) 寻找异常点 

(4) 计算距离  两个观测值之间最常用的距离量度是欧几里得距离 

(5) 选择聚类算法  层次聚类/划分聚类

(6) 获得一种或多种聚类方法 

(7) 确定类的数目 

(8) 获得最终的聚类解决方案 

(9) 结果可视化 

(10) 解读类 

(11) 验证结果 

&nbsp;

##  计算距离

![](https://i.loli.net/2018/09/20/5ba336cbd341c.jpg)

~~~R
d <- dist(nutrient) ###计算距离，默认是欧式距离 结果为一个下三角矩阵
as.matrix(d)[1:4,1:4] ###转化为matrix
~~~

&nbsp;

##  层次聚类

(1) 定义每个观测值（行或单元）为一类； 

(2) 计算每类和其他各类的距离；

(3) 把距离最短的两类合并成一类，这样类的个数就减少一个；

(4) 重复步骤(2)和步骤(3)，直到包含所有观测值的类合并成单个的类为止 

&nbsp;

![](https://i.loli.net/2018/09/20/5ba337bd9cfc1.jpg)

&nbsp;

~~~R
data(nutrient, package="flexclust")
row.names(nutrient) <- tolower(row.names(nutrient)) ##标签转化为小写
nutrient.scaled <- scale(nutrient) 
d <- dist(nutrient.scaled)
fit.average <- hclust(d, method="average")
plot(fit.average, hang=-1, cex=.8, main="Average Linkage Clustering")
~~~

&nbsp;

层次聚类实现: `hclust`

&nbsp;

###  选择聚类的个数

~~~R
> library(NbClust)
> devAskNewPage(ask=TRUE)
> nc <- NbClust(nutrient.scaled, distance="euclidean", min.nc=2, max.nc=15,    method="average") ##distance 计算距离的方法  euclidean欧式距离
> table(nc$Best.n[1,])
	#0 2 3 4 5 9 10 13 14 15
	#2 4 4 3 4 1 1 2 1 4
> barplot(table(nc$Best.n[1,]), xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")# 获取最终的聚类方案 
~~~

&nbsp;

###  获取最终的聚类方案 

~~~R
> clusters <- cutree(fit.average, k=5)
> table(clusters)
clusters
1 2 3 4 5
7 16 1 2 1
> aggregate(nutrient, by=list(cluster=clusters), median) ##描述聚类
> plot(fit.average, hang=-1, cex=.8, main="Average Linkage Clustering\n5 Cluster Solution")
> rect.hclust(fit.average, k=5) ## 结果绘图
~~~

`aggregate`  Splits the data into subsets, computes summary statistics for each, and returns the result in a convenient form

&nbsp;

##  划分聚类分析

 K均值和基于中心点的划分（ PAM）

###  K 均值聚类 

(1) 选择K个中心点（随机选择K行）；

(2) 把每个数据点分配到离它最近的中心点；

(3) 重新计算每类中的点到该类中心点距离的平均值（也就说，得到长度为p的均值向量，这
里的p是变量的个数）；

(4) 分配每个数据到它最近的中心点； 

(5) 重复步骤(3)和步骤(4)直到所有的观测值不再被分配或是达到最大的迭代次数（ R把10次
作为默认迭代次数）。 

&nbsp;

K均值聚类能处理比层次聚类更大的数据集。另外，观测值不会永远被分到一类中。当我们
提高整体解决方案时，聚类方案也会改动。但是均值的使用意味着所有的变量必须是连续的，并
且这个方法很有可能被异常值影响。它在非凸聚类（例如U型）情况下也会变得很差。 

 &nbsp;

