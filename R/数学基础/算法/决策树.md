#  决策树

实际上是寻找最纯净的划分方法。在数学上叫做纯度。即目标变量分的足够开



决策树的判定过程就相当于树中从根结点到某一个叶子结点的遍历。每一步如何遍历是由数据各个特征的具体特征属性决定。



##  如何构建决策树

每一次子结点的产生，是由于在当前层数选择了不同的特征来作为我的分裂因素造成的

如何选择特征===>分裂属性

所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。 



这就需要:ID3算法，C4.5算法以及CART算法 



##  条件熵

条件熵H（Y|X）表示在已知随机变量X的条件下随机变量Y的不确定性 

条件熵意思是按一个新的变量的每个值对原变量进行分类 。然后在每一个小类里面，都计算一个小熵，然后每一个小熵乘以各个类别的概率，然后求和。 

信息增益：用另一个变量对原变量分类后，原变量的不确定性就会减小了，因为新增了Y的信息。不确定程度减少了多少就是信息的增益。 用另一个变量对原变量分类后，原变量的不确定性就会减小了，因为新增了Y的信息，不确定程度减少了多少就是信息的增益。 

H（Y|X=长相） = p(X =帅)\*H（Y|X=帅）+p(X =不帅)\*H（Y|X=不帅） 



ID3算法就是在每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的属性进行分裂。

ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。ID3的后继算法C4.5使用增益率（gain ratio）的信息增益扩充，试图克服这个偏倚。



##  剪枝

 在实际构造决策树时，通常要进行[剪枝](http://en.wikipedia.org/wiki/Pruning_(decision_trees))，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：

​      先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。

​      后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝。  