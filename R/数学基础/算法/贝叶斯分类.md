# 贝叶斯分类

朴素贝叶斯的思想严格的定义是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。其中在求解某个待分类项出现的条件下，各个类别出现的概率时，一般是根据该待分类项的特征属性来求的。



 第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。

      第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。
    
      第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。

![img](http://images.cnblogs.com/cnblogs_com/leoo2sk/WindowsLiveWriter/4f6168bb064a_9C14/1_2.png)





  #  BIC

wiki:  In [statistics](https://en.wikipedia.org/wiki/Statistics), the **Bayesian information criterion** (**BIC**) or **Schwarz criterion** (also **SBC**, **SBIC**) is a criterion for [model selection](https://en.wikipedia.org/wiki/Model_selection)among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function) and it is closely related to the [Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) (AIC).

When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in [overfitting](https://en.wikipedia.org/wiki/Overfitting). Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.

The BIC was developed by Gideon E. Schwarz and published in a 1978 paper,[[1\]](https://en.wikipedia.org/wiki/Bayesian_information_criterion#cite_note-1) where he gave a [Bayesian](https://en.wikipedia.org/wiki/Bayesian_inference) argument for adopting it.

AIC和BIC主要用于模型的选择，AIC、BIC越小越好。
在对不同模型进行比较时，AIC、BIC降低越多，说明该模型的拟合效果越好；



